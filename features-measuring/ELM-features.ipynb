{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7923855,"sourceType":"datasetVersion","datasetId":4656784},{"sourceId":7927184,"sourceType":"datasetVersion","datasetId":4658905},{"sourceId":7931395,"sourceType":"datasetVersion","datasetId":4661991}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install snownlp","metadata":{"execution":{"iopub.status.busy":"2024-03-24T16:23:26.513521Z","iopub.execute_input":"2024-03-24T16:23:26.514066Z","iopub.status.idle":"2024-03-24T16:23:55.707814Z","shell.execute_reply.started":"2024-03-24T16:23:26.514027Z","shell.execute_reply":"2024-03-24T16:23:55.706267Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Collecting snownlp\n  Downloading snownlp-0.12.3.tar.gz (37.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.6/37.6 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: snownlp\n  Building wheel for snownlp (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for snownlp: filename=snownlp-0.12.3-py3-none-any.whl size=37760944 sha256=a65424e8842d4f6bdde107dfa62a7fc8068e975b4237e7ca98789f783e280f0b\n  Stored in directory: /root/.cache/pip/wheels/43/f3/70/8990fc249efeb396007766676706f71dd3d1ca3c023ce522ce\nSuccessfully built snownlp\nInstalling collected packages: snownlp\nSuccessfully installed snownlp-0.12.3\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport math\nimport os\nimport re\nimport jieba\nimport jieba.posseg as pseg  \nfrom snownlp import SnowNLP ","metadata":{"execution":{"iopub.status.busy":"2024-03-24T16:24:21.869050Z","iopub.execute_input":"2024-03-24T16:24:21.869537Z","iopub.status.idle":"2024-03-24T16:24:21.878209Z","shell.execute_reply.started":"2024-03-24T16:24:21.869501Z","shell.execute_reply":"2024-03-24T16:24:21.876394Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"#读取停用词\nfolder_path = \"/kaggle/input/stopwords\"\nstopwords = []\ntry:\n    for file_name in os.listdir(folder_path):\n        if file_name.endswith(\".txt\"):\n            file_path = os.path.join(folder_path, file_name)\n            with open(file_path, 'r', encoding='utf-8') as file:\n                s = file.readlines()\n                # print(f\"文件名: {file_name}\")\n                stopwords+=s\n                \n                # print(\"----------------------\")\nexcept FileNotFoundError:\n    print(\"找不到指定的文件夹，请检查文件夹路径是否正确。\")\nexcept Exception as e:\n    print(\"发生错误:\", e)\nstopwords = [word.replace('\\n', '') for word in stopwords]\n# print(stopwords)","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-03-24T16:02:29.727570500Z","start_time":"2024-03-24T16:02:29.195800300Z"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-24T16:24:31.197286Z","iopub.execute_input":"2024-03-24T16:24:31.197767Z","iopub.status.idle":"2024-03-24T16:24:31.224868Z","shell.execute_reply.started":"2024-03-24T16:24:31.197728Z","shell.execute_reply":"2024-03-24T16:24:31.223856Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# 医学术语列表\ndef Medicalwordslist(filepath2):  # 创建专业医学词汇列表\n    pattern = re.compile(r'[^\\u4e00-\\u9fa5]')\n    Medicalword = [re.sub(pattern, \"\", line.strip().replace(\" \", '')) for line in open(filepath2, 'r', encoding=\"utf-8\").readlines()]  # 以行的形式读取停用词表，同时转换为列表\n    return Medicalword\nmedicalword = Medicalwordslist(r'/kaggle/input/medical-terms/all_medical_terms.txt')","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-03-24T16:02:30.985416Z","start_time":"2024-03-24T16:02:29.220214100Z"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-24T16:24:36.717612Z","iopub.execute_input":"2024-03-24T16:24:36.718002Z","iopub.status.idle":"2024-03-24T16:24:38.980439Z","shell.execute_reply.started":"2024-03-24T16:24:36.717970Z","shell.execute_reply":"2024-03-24T16:24:38.979030Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"特征提取函数","metadata":{}},{"cell_type":"code","source":"# 文本长度\ndef count_text_length(text):\n    return len(text)","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-03-24T16:02:31.001012700Z","start_time":"2024-03-24T16:02:30.990421600Z"},"jupyter":{"outputs_hidden":false}},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"#句子数\ndef count_num_sentences(text):\n    # 使用正则表达式匹配中文句号、问号、感叹号作为句子的结束符号\n    pattern = r'[\\u4e00-\\u9fa5][。？！]'\n    sentences = re.findall(pattern, text)\n    \n    # 最后一句可能没有结束符号，需要额外判断\n    if len(text) > 0 or text[-1] in '。？！':\n        sentences.append(text[-1])\n    \n    return len(sentences)","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-03-24T16:02:31.060085400Z","start_time":"2024-03-24T16:02:30.998972900Z"},"jupyter":{"outputs_hidden":false}},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"# 单词数量\ndef count_num_words(text):\n    # 使用正则表达式去除标点符号\n    text = re.sub(r'[^\\u4e00-\\u9fa5]', '', text)\n   \n    # 使用结巴分词对中文文本进行分词\n    words = jieba.lcut(text, cut_all=True)\n    \n    # 去除空格等无意义字符\n    words = [word for word in words if word.strip()]\n\n    return len(words)\n","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-03-24T16:02:31.060784400Z","start_time":"2024-03-24T16:02:31.030542100Z"},"jupyter":{"outputs_hidden":false}},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"#停用词数\ndef count_num_stopwords(text,stopwords=stopwords):\n    # 使用正则表达式去除标点符号\n    text = re.sub(r'[^\\u4e00-\\u9fa5]', '', text)   \n    # 使用结巴分词对中文文本进行分词\n    words = jieba.lcut(text, cut_all=True)    \n    # 去除空格等无意义字符\n    words = [word for word in words if word.strip()]\n    # 计算停用词数\n    stop_words = [word for word in words if word  in stopwords]\n    return len(stop_words)","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-03-24T16:06:44.075591200Z","start_time":"2024-03-24T16:06:44.051649600Z"},"jupyter":{"outputs_hidden":false}},"execution_count":110,"outputs":[]},{"cell_type":"code","source":"# 标点符号数\ndef count_num_punctuation(text):  \n    # 常见的中文标点符号列表  \n    chinese_punctuation = '，。！？；：“”、（）《》「」【】『』｛｝……'  \n      \n    # 初始化计数器  \n    count = 0  \n      \n    # 遍历文本的每个字符  \n    for char in text:  \n        # 如果字符在中文标点符号列表中，则计数器加1  \n        if char in chinese_punctuation:  \n            count += 1  \n      \n    # 返回标点符号数量  \n    return count  ","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-03-24T16:02:31.111965700Z","start_time":"2024-03-24T16:02:31.058348200Z"},"jupyter":{"outputs_hidden":false}},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"# 短句数量\ndef count_num_shortsentences(text, max_length=20):  \n    # 句末标点符号列表  \n    sentence_ending_punctuation = '，。！？；：“”、（）《》「」【】『』｛｝……'  \n      \n    # 使用正则表达式来匹配句末标点符号，并分割文本为句子列表  \n    sentences = re.split(r'[{}]+'.format(re.escape(sentence_ending_punctuation)), text)  \n    sentences=sentences[:-1]\n    # 过滤并计数字符数在max_length以下的句子  \n    short_sentence_count = sum([1 for s in sentences if len(s.strip()) <= max_length])  \n      \n    # 返回短句数量  \n    return short_sentence_count  \n  ","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-03-24T16:06:54.150961300Z","start_time":"2024-03-24T16:06:54.133309Z"},"jupyter":{"outputs_hidden":false}},"execution_count":111,"outputs":[]},{"cell_type":"code","source":"# 形容词数\ndef count_num_adjectives(text):  \n    # 使用jieba的词性标注功能  \n    words = pseg.cut(text)  \n    \n    # 定义形容词的词性标签集合  \n    # 'a' 代表形容词  \n    adjective_tags = ['a', 'ad', 'an', 'ag', 'al']  \n      \n    # 初始化形容词计数器  \n    adjective_count = 0  \n      \n    # 遍历每个词和对应的词性  \n    for word, flag in words:  \n        # 如果词性标签在形容词标签集合中，则增加计数器  \n        if flag in adjective_tags:  \n            adjective_count += 1  \n      \n    # 返回形容词数量  \n    return adjective_count  \n  ","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-03-24T16:02:31.142977500Z","start_time":"2024-03-24T16:02:31.093702700Z"},"jupyter":{"outputs_hidden":false}},"execution_count":93,"outputs":[]},{"cell_type":"code","source":"# 副词数\ndef count_num_adverbs(text):  \n    # 使用jieba的词性标注功能  \n    words = pseg.cut(text)  \n      \n    # 定义副词的词性标签  \n    adverb_tags = ['d', 'df', 'dg']\n      \n    # 初始化副词计数器  \n    adverb_count = 0  \n      \n    # 遍历每个词和对应的词性  \n    for word, flag in words:  \n        # 如果词性标签是副词标签，则增加计数器  \n        if flag in adverb_tags:  \n            adverb_count += 1  \n      \n    # 返回副词数量  \n    return adverb_count  ","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-03-24T16:02:31.169180800Z","start_time":"2024-03-24T16:02:31.113369900Z"},"jupyter":{"outputs_hidden":false}},"execution_count":94,"outputs":[]},{"cell_type":"code","source":"# 名词数\ndef count_num_nouns(text):  \n    # 使用jieba进行分词和词性标注  \n    words = pseg.cut(text)\n    \n    noun_count = 0  \n    noun = ['n', 'nr', 'nr1', 'nr2', 'nrj', 'ns', 'nsf', 'nt', 'nz', 'nl', 'nx', 'ng', 'nrt', 'nrfg']\n    # 遍历分词结果，统计名词数量  \n    for word, flag in words:  \n        # 'n' 在jieba中表示名词  \n        if flag in noun:  \n            noun_count += 1  \n      \n    return noun_count  \n","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-03-24T16:02:31.206220700Z","start_time":"2024-03-24T16:02:31.132240800Z"},"jupyter":{"outputs_hidden":false}},"execution_count":95,"outputs":[]},{"cell_type":"code","source":"# 动词数\ndef count_num_verbs(text):  \n    # 使用jieba的词性标注功能  \n    words = pseg.cut(text)  \n   \n    # 定义动词的词性标签  \n    verb = ['v', 'vd', 'vg', 'vi', 'vn', 'vq', 'vshi', 'vyou', 'vf', 'vx', 'vl']\n    \n    # 初始化动词计数器  \n    verb_count = 0  \n      \n    # 遍历每个词和对应的词性  \n    for word, flag in words:  \n        # 如果词性标签是动词标签，则增加计数器  \n        if flag in verb:  \n            verb_count += 1  \n      \n    # 返回动词数量  \n    return verb_count  ","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-03-24T16:02:31.210807700Z","start_time":"2024-03-24T16:02:31.145976200Z"},"jupyter":{"outputs_hidden":false}},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"# 答案关键词数\t\ndef count_num_keywords(text, stopwords=stopwords):\n    # 使用正则表达式去除标点符号\n    text = re.sub(r'[^\\u4e00-\\u9fa5]', '', text)\n    \n    # 使用结巴分词对中文文本进行分词\n    words = jieba.lcut(text,cut_all=True)\n    \n    # 去除停用词\n    words = [word for word in words if word.strip() and word not in stopwords]\n    \n    return len(words)","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-03-24T16:02:31.299429100Z","start_time":"2024-03-24T16:02:31.159586400Z"},"jupyter":{"outputs_hidden":false}},"execution_count":97,"outputs":[]},{"cell_type":"code","source":"# 文本信息熵\ndef calculate_entropy(text,stopwords=stopwords):\n    # 使用正则表达式去除标点符号\n    text = re.sub(r'[^\\u4e00-\\u9fa5]', '', text)\n    # 使用结巴分词对中文文本进行分词\n    words = jieba.lcut(text, cut_all=True)\n    # 去除停用词\n    words = list(set([word for word in words if word.strip() and word not in stopwords]))\n    \n    char_freq = {}\n    total_chars = 0\n    entropy = 0.0\n\n    # 统计每个字符出现的频率\n    for char in words:\n        if char in char_freq:\n            char_freq[char] += 1\n        else:\n            char_freq[char] = 1\n        total_chars += 1\n\n    # 计算信息熵\n    for freq in char_freq.values():\n        probability = freq / total_chars\n        entropy -= probability * math.log(probability, 2)\n\n    return round(entropy,4)","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-03-24T16:02:31.299429100Z","start_time":"2024-03-24T16:02:31.175387200Z"},"jupyter":{"outputs_hidden":false}},"execution_count":98,"outputs":[]},{"cell_type":"code","source":"# 医学术语数\ndef count_num_medicalwords(text,stopwords=stopwords,medicalword=medicalword):\n    Medicalword =medicalword\n    # 去除标点符号和停用词\n    text = re.sub(r'[^\\u4e00-\\u9fa5]', '', text)\n    \n    # 使用结巴分词对中文文本进行分词\n    words = jieba.lcut(text, cut_all=True)\n    \n    # 去除停用词\n    words = list(set([word for word in words if word.strip() and word not in stopwords]))\n    \n    medicalwords = []  # 医学专业词的总长度\n    for word in words:\n        if word in Medicalword: \n            # print(\"word:\", word)\n            medicalwords.append(word)\n    \n    return len(medicalwords)","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-03-24T16:07:55.704770500Z","start_time":"2024-03-24T16:07:55.692108Z"},"jupyter":{"outputs_hidden":false}},"execution_count":112,"outputs":[]},{"cell_type":"code","source":"# 医学术语与文本关键词数比值\ndef density_medicalwords(num_keywords,num_medicalwords):\n    \n    return round(num_medicalwords/num_keywords,3)\n    # 去除停用词 ","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-03-24T16:02:31.333893700Z","start_time":"2024-03-24T16:02:31.219630400Z"},"jupyter":{"outputs_hidden":false}},"execution_count":100,"outputs":[]},{"cell_type":"code","source":"# 问题和答案关键词重合度\ndef extract_keywords(text,stopwords=stopwords):  \n    # 使用正则表达式去除标点符号\n    text = re.sub(r'[^\\u4e00-\\u9fa5]', '', text)\n    \n    # 使用结巴分词对中文文本进行分词\n    words = jieba.lcut(text,cut_all=True)\n    \n    # 去除停用词\n    words = [word for word in words if word.strip() and word not in stopwords]\n    # print(words)\n    return words\n  \ndef calculate_keyword_overlap(text1, text2):  \n    \"\"\"  \n    计算两段文本的关键词重合度  \n    :param text1: 第一段文本  \n    :param text2: 第二段文本  \n    :return: 关键词重合度  \n    \"\"\"  \n    # 提取关键词  \n    keywords1 = extract_keywords(text1)  \n    keywords2 = extract_keywords(text2)  \n      \n    # 计算重合的关键词数量  \n    overlap = len(set(keywords1) & set(keywords2))  \n      \n    # 计算总的关键词数量（去除重复）  \n    total = len(set(keywords1) | set(keywords2))  \n      \n    # 计算重合度  \n    overlap_rate = overlap / total if total > 0 else 0  \n      \n    return round(overlap_rate,3)  ","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-03-24T16:02:31.356787900Z","start_time":"2024-03-24T16:02:31.235592300Z"},"jupyter":{"outputs_hidden":false}},"execution_count":101,"outputs":[]},{"cell_type":"code","source":"# 问题和答案语义相似性\ndef calculate_semantic_similarity():\n    return 0","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-03-24T16:02:31.357816500Z","start_time":"2024-03-24T16:02:31.248784300Z"},"jupyter":{"outputs_hidden":false}},"execution_count":102,"outputs":[]},{"cell_type":"code","source":"# 积极情感词数\ndef count_num_positive_words(text,stopwords=stopwords):\n    # 去除标点符号和停用词\n    text = re.sub(r'[^\\u4e00-\\u9fa5]', '', text)\n        \n    # 使用结巴分词对中文文本进行分词\n    words = jieba.lcut(text)\n    # 去除停用词\n    words = [word for word in words if word.strip() and word not in stopwords]\n    \n    # 初始化积极情感词计数器  \n    positive_word_count = 0  \n      \n    # 遍历每个词，尝试进行情感分析  \n    for word in words:  \n        # 注意：这里假设SnowNLP的sentiment方法返回的情感分数越高，表示越积极  \n        # 但实际上，SnowNLP的sentiment方法通常用于判断整个句子的情感倾向，而不是单个词  \n        sentiment_score = SnowNLP(word).sentiments \n          \n        # 设定一个阈值，认为情感分数大于这个阈值的词为积极情感词  \n        threshold = 0.5  # 这个阈值需要根据实际情况进行调整  \n        if sentiment_score > threshold:\n            # print(word)\n            positive_word_count += 1  \n    \n    return positive_word_count","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-03-24T16:09:09.665529300Z","start_time":"2024-03-24T16:09:09.601605100Z"},"jupyter":{"outputs_hidden":false}},"execution_count":118,"outputs":[]},{"cell_type":"code","source":"# 消极情感词数\ndef count_num_negative_words(text,stopwords=stopwords):\n    text = re.sub(r'[^\\u4e00-\\u9fa5]', '', text)\n        \n    # 使用结巴分词对中文文本进行分词\n    words = jieba.lcut(text)\n    # 去除停用词\n    words = [word for word in words if word.strip() and word not in stopwords]\n    \n    # 初始化积极情感词计数器  \n    negative_word_count = 0  \n      \n    # 遍历每个词，尝试进行情感分析  \n    for word in words:  \n        # 注意：这里假设SnowNLP的sentiment方法返回的情感分数越高，表示越积极  \n        # 但实际上，SnowNLP的sentiment方法通常用于判断整个句子的情感倾向，而不是单个词  \n        sentiment_score = SnowNLP(word).sentiments \n          \n        # 设定一个阈值，认为情感分数大于这个阈值的词为积极情感词  \n        threshold = 0.5  # 这个阈值需要根据实际情况进行调整  \n        if sentiment_score < threshold:\n            # print(word)\n            negative_word_count += 1  \n    \n    return negative_word_count","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-03-24T16:09:04.575776100Z","start_time":"2024-03-24T16:09:04.542685900Z"},"jupyter":{"outputs_hidden":false}},"execution_count":117,"outputs":[]},{"cell_type":"code","source":"# 情感极性\ndef calculate_sentiment_polarity(text):\n    text = re.sub(r'[^\\u4e00-\\u9fa5]', '', text)\n    s= SnowNLP(text)\n    \n    return round(s.sentiments,5)","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-03-24T16:08:15.589491200Z","start_time":"2024-03-24T16:08:15.560981Z"},"jupyter":{"outputs_hidden":false}},"execution_count":115,"outputs":[]},{"cell_type":"markdown","source":"特征量化","metadata":{}},{"cell_type":"code","source":"#函数进行特征提取\ndef feature_extraction(question,answer):\n    #文本长度\n    len_text=count_text_length(answer)\n    #句子数\n    num_sentences=count_num_sentences(answer)\n    #单词数\n    num_words=count_num_words(answer)\n    #停用词数量\n    num_stopwords=count_num_stopwords(answer)\n    #标点符号数\n    num_punctuations=count_num_punctuation(answer)\n    #短句数\n    num_short_sentences=count_num_shortsentences(answer)\n    #形容词数\n    num_adjectives=count_num_adjectives(answer)\n    #动词数\n    num_verbs=count_num_verbs(answer)\n    #名词数\n    num_nouns=count_num_nouns(answer)\n    #动词数\n    num_adverbs=count_num_adverbs(answer)\n    #关键词数\n    num_keywords=count_num_keywords(answer)\n    #文本信息熵\n    text_entropy=calculate_entropy(answer)\n    #医学术语数量\n    num_medical_terms=count_num_medicalwords(answer)\n    #医学术语与关键词比例\n    medical_keywords_ratio=density_medicalwords(num_keywords,num_medical_terms)\n    #问题和答案关键词重合度\n    keywords_overlap=calculate_keyword_overlap(question,answer)\n    # 语义相似度\n    semantic_similarity=calculate_semantic_similarity()\n    # 积极情感词数\n    num_positive_words=count_num_positive_words(answer)\n    # 消极情感词数\n    num_native_words=count_num_negative_words(answer)\n    # 情感极性\n    sentiment_polarity=calculate_sentiment_polarity(answer)\n    \n    return [len_text,num_sentences,num_words,num_stopwords,num_punctuations,num_short_sentences,num_adjectives,num_verbs,num_nouns,num_adverbs,num_keywords,text_entropy,num_medical_terms,medical_keywords_ratio,keywords_overlap,semantic_similarity,num_positive_words,num_native_words,sentiment_polarity]","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-03-24T16:02:31.428803500Z","start_time":"2024-03-24T16:02:31.318391900Z"},"jupyter":{"outputs_hidden":false}},"execution_count":106,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/chatgpt/chatGPT.csv')\n# data.head()","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-03-24T16:02:47.022167400Z","start_time":"2024-03-24T16:02:31.337139300Z"},"jupyter":{"outputs_hidden":false}},"execution_count":107,"outputs":[{"execution_count":107,"output_type":"execute_result","data":{"text/plain":"   序号                 问题                                               医生回答  \\\n0   1             屁股能吸脂吗  屁股一般能吸脂，可以改善屁股比较肥胖的情况。如果屁股比较肥胖，有可能会影响到美观，通过气质的...   \n1   2         女贞子药用价值有哪些  女贞子具有苦甘平性，归属于肝肾经。它的主要功效是补充肝肾，强健腰脊，可治疗阴虚内热、头晕眼花...   \n2   3          哺乳期来月经减肥吗  哺乳期是否来月经，以及来月经的时间与减肥没有任何关系。一般情况下月经是否来潮与期间泌乳素水平...   \n3   4  太阳凹陷填充胶原蛋白和玻尿酸哪个好  太阳凹陷填充胶原蛋白和玻尿酸都是比较好的，都可以达到填充的效果。胶原蛋白填充太阳穴通常可以达...   \n4   5         伤到筋骨用什么药最好  川续断主要产于四川，湖北等地，其中四川的续断质量最好，所以被称之为川续断。川续断具有补肝肾，...   \n\n                                           ChatGPT回答   医生    科室  \n0  屁股可以通过脂肪吸脂手术来减肥。吸脂手术是一种通过利用特殊工具将过多的脂肪组织抽出的方法。然...  吕仁荣  整形美容  \n1  女贞子是一种常见的中药材，具有多种药用价值。以下是女贞子的一些主要药用价值：1. 温肾壮阳：...   王燕   中医科  \n2  在哺乳期来月经后减肥是可能的，因为月经结束后，女性身体恢复了正常的激素水平，消耗能量和脂肪更...  赵晓东   妇产科  \n3  太阳凹陷是由于皮肤老化、流失胶原蛋白和玻尿酸等造成的。填充胶原蛋白和玻尿酸都可以帮助恢复皮肤...  章庆国  整形美容  \n4  如果伤到了筋骨，最好寻求医生的帮助进行评估和治疗。 医生可能会建议您使用以下药物来缓解疼痛和...  陈守新    其他  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>序号</th>\n      <th>问题</th>\n      <th>医生回答</th>\n      <th>ChatGPT回答</th>\n      <th>医生</th>\n      <th>科室</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>屁股能吸脂吗</td>\n      <td>屁股一般能吸脂，可以改善屁股比较肥胖的情况。如果屁股比较肥胖，有可能会影响到美观，通过气质的...</td>\n      <td>屁股可以通过脂肪吸脂手术来减肥。吸脂手术是一种通过利用特殊工具将过多的脂肪组织抽出的方法。然...</td>\n      <td>吕仁荣</td>\n      <td>整形美容</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>女贞子药用价值有哪些</td>\n      <td>女贞子具有苦甘平性，归属于肝肾经。它的主要功效是补充肝肾，强健腰脊，可治疗阴虚内热、头晕眼花...</td>\n      <td>女贞子是一种常见的中药材，具有多种药用价值。以下是女贞子的一些主要药用价值：1. 温肾壮阳：...</td>\n      <td>王燕</td>\n      <td>中医科</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>哺乳期来月经减肥吗</td>\n      <td>哺乳期是否来月经，以及来月经的时间与减肥没有任何关系。一般情况下月经是否来潮与期间泌乳素水平...</td>\n      <td>在哺乳期来月经后减肥是可能的，因为月经结束后，女性身体恢复了正常的激素水平，消耗能量和脂肪更...</td>\n      <td>赵晓东</td>\n      <td>妇产科</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>太阳凹陷填充胶原蛋白和玻尿酸哪个好</td>\n      <td>太阳凹陷填充胶原蛋白和玻尿酸都是比较好的，都可以达到填充的效果。胶原蛋白填充太阳穴通常可以达...</td>\n      <td>太阳凹陷是由于皮肤老化、流失胶原蛋白和玻尿酸等造成的。填充胶原蛋白和玻尿酸都可以帮助恢复皮肤...</td>\n      <td>章庆国</td>\n      <td>整形美容</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>伤到筋骨用什么药最好</td>\n      <td>川续断主要产于四川，湖北等地，其中四川的续断质量最好，所以被称之为川续断。川续断具有补肝肾，...</td>\n      <td>如果伤到了筋骨，最好寻求医生的帮助进行评估和治疗。 医生可能会建议您使用以下药物来缓解疼痛和...</td>\n      <td>陈守新</td>\n      <td>其他</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df=pd.DataFrame(columns=['len_text','num_sentences','num_words','num_stopwords','num_punctuations','num_short_sentences','num_adjectives','num_verbs','num_nouns','num_adverbs','num_keywords','text_entropy','num_medical_terms','medical_keywords_ratio','keywords_overlap','semantic_similarity','num_positive_words','num_native_words','sentiment_polarity'])\nfor i in range(len(data)):\n    df.loc[i]=feature_extraction(data['问题'][i],data['ChatGPT回答'][i])\ndf.to_csv('chatGPT_data_feature.csv')","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-03-24T16:09:21.635996Z","start_time":"2024-03-24T16:09:15.467397700Z"},"jupyter":{"outputs_hidden":false}},"execution_count":119,"outputs":[{"execution_count":119,"output_type":"execute_result","data":{"text/plain":"   len_text  num_sentences  num_words  num_stopwords  num_punctuations  \\\n0     167.0            6.0       98.0           42.0              12.0   \n1     398.0            8.0      236.0           64.0              31.0   \n2     304.0            7.0      166.0           71.0              25.0   \n3     392.0           12.0      229.0           84.0              30.0   \n4     343.0            7.0      161.0           56.0              32.0   \n5     260.0            8.0      151.0           59.0              19.0   \n6     399.0           12.0      219.0           90.0              32.0   \n7     433.0           13.0      239.0           92.0              39.0   \n8     273.0           10.0      162.0           66.0              24.0   \n9     122.0            4.0       73.0           31.0               7.0   \n\n   num_short_sentences  num_adjectives  num_verbs  num_nouns  num_adverbs  \\\n0                 11.0             3.0       16.0       30.0          3.0   \n1                 29.0             8.0       45.0       71.0          2.0   \n2                 25.0             7.0       54.0       42.0          6.0   \n3                 26.0             4.0       51.0       69.0          8.0   \n4                 26.0             5.0       46.0       44.0          1.0   \n5                 17.0             6.0       45.0       37.0          6.0   \n6                 31.0            10.0       59.0       57.0          3.0   \n7                 39.0             6.0       71.0       51.0          7.0   \n8                 22.0             5.0       38.0       31.0          7.0   \n9                  5.0             7.0       16.0       17.0          3.0   \n\n   num_keywords  text_entropy  num_medical_terms  medical_keywords_ratio  \\\n0          56.0        5.3576                2.0                   0.036   \n1         172.0        6.7142               17.0                   0.099   \n2          95.0        6.0661                2.0                   0.021   \n3         145.0        6.3219               10.0                   0.069   \n4         105.0        6.0661                7.0                   0.067   \n5          92.0        6.1085                3.0                   0.033   \n6         129.0        6.7279                6.0                   0.047   \n7         147.0        6.7415                2.0                   0.014   \n8          96.0        6.1898                4.0                   0.042   \n9          42.0        5.0000                1.0                   0.024   \n\n   keywords_overlap  semantic_similarity  num_positive_words  \\\n0             0.073                  0.0                29.0   \n1             0.048                  0.0                55.0   \n2             0.060                  0.0                38.0   \n3             0.100                  0.0                51.0   \n4             0.029                  0.0                51.0   \n5             0.087                  0.0                52.0   \n6             0.019                  0.0                50.0   \n7             0.019                  0.0                57.0   \n8             0.039                  0.0                23.0   \n9             0.094                  0.0                15.0   \n\n   num_native_words  sentiment_polarity  \n0              12.0             1.00000  \n1              30.0             1.00000  \n2              28.0             1.00000  \n3              39.0             0.00004  \n4              17.0             1.00000  \n5              13.0             1.00000  \n6              43.0             0.33186  \n7              46.0             1.00000  \n8              38.0             0.00022  \n9               7.0             0.95624  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>len_text</th>\n      <th>num_sentences</th>\n      <th>num_words</th>\n      <th>num_stopwords</th>\n      <th>num_punctuations</th>\n      <th>num_short_sentences</th>\n      <th>num_adjectives</th>\n      <th>num_verbs</th>\n      <th>num_nouns</th>\n      <th>num_adverbs</th>\n      <th>num_keywords</th>\n      <th>text_entropy</th>\n      <th>num_medical_terms</th>\n      <th>medical_keywords_ratio</th>\n      <th>keywords_overlap</th>\n      <th>semantic_similarity</th>\n      <th>num_positive_words</th>\n      <th>num_native_words</th>\n      <th>sentiment_polarity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>167.0</td>\n      <td>6.0</td>\n      <td>98.0</td>\n      <td>42.0</td>\n      <td>12.0</td>\n      <td>11.0</td>\n      <td>3.0</td>\n      <td>16.0</td>\n      <td>30.0</td>\n      <td>3.0</td>\n      <td>56.0</td>\n      <td>5.3576</td>\n      <td>2.0</td>\n      <td>0.036</td>\n      <td>0.073</td>\n      <td>0.0</td>\n      <td>29.0</td>\n      <td>12.0</td>\n      <td>1.00000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>398.0</td>\n      <td>8.0</td>\n      <td>236.0</td>\n      <td>64.0</td>\n      <td>31.0</td>\n      <td>29.0</td>\n      <td>8.0</td>\n      <td>45.0</td>\n      <td>71.0</td>\n      <td>2.0</td>\n      <td>172.0</td>\n      <td>6.7142</td>\n      <td>17.0</td>\n      <td>0.099</td>\n      <td>0.048</td>\n      <td>0.0</td>\n      <td>55.0</td>\n      <td>30.0</td>\n      <td>1.00000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>304.0</td>\n      <td>7.0</td>\n      <td>166.0</td>\n      <td>71.0</td>\n      <td>25.0</td>\n      <td>25.0</td>\n      <td>7.0</td>\n      <td>54.0</td>\n      <td>42.0</td>\n      <td>6.0</td>\n      <td>95.0</td>\n      <td>6.0661</td>\n      <td>2.0</td>\n      <td>0.021</td>\n      <td>0.060</td>\n      <td>0.0</td>\n      <td>38.0</td>\n      <td>28.0</td>\n      <td>1.00000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>392.0</td>\n      <td>12.0</td>\n      <td>229.0</td>\n      <td>84.0</td>\n      <td>30.0</td>\n      <td>26.0</td>\n      <td>4.0</td>\n      <td>51.0</td>\n      <td>69.0</td>\n      <td>8.0</td>\n      <td>145.0</td>\n      <td>6.3219</td>\n      <td>10.0</td>\n      <td>0.069</td>\n      <td>0.100</td>\n      <td>0.0</td>\n      <td>51.0</td>\n      <td>39.0</td>\n      <td>0.00004</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>343.0</td>\n      <td>7.0</td>\n      <td>161.0</td>\n      <td>56.0</td>\n      <td>32.0</td>\n      <td>26.0</td>\n      <td>5.0</td>\n      <td>46.0</td>\n      <td>44.0</td>\n      <td>1.0</td>\n      <td>105.0</td>\n      <td>6.0661</td>\n      <td>7.0</td>\n      <td>0.067</td>\n      <td>0.029</td>\n      <td>0.0</td>\n      <td>51.0</td>\n      <td>17.0</td>\n      <td>1.00000</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>260.0</td>\n      <td>8.0</td>\n      <td>151.0</td>\n      <td>59.0</td>\n      <td>19.0</td>\n      <td>17.0</td>\n      <td>6.0</td>\n      <td>45.0</td>\n      <td>37.0</td>\n      <td>6.0</td>\n      <td>92.0</td>\n      <td>6.1085</td>\n      <td>3.0</td>\n      <td>0.033</td>\n      <td>0.087</td>\n      <td>0.0</td>\n      <td>52.0</td>\n      <td>13.0</td>\n      <td>1.00000</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>399.0</td>\n      <td>12.0</td>\n      <td>219.0</td>\n      <td>90.0</td>\n      <td>32.0</td>\n      <td>31.0</td>\n      <td>10.0</td>\n      <td>59.0</td>\n      <td>57.0</td>\n      <td>3.0</td>\n      <td>129.0</td>\n      <td>6.7279</td>\n      <td>6.0</td>\n      <td>0.047</td>\n      <td>0.019</td>\n      <td>0.0</td>\n      <td>50.0</td>\n      <td>43.0</td>\n      <td>0.33186</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>433.0</td>\n      <td>13.0</td>\n      <td>239.0</td>\n      <td>92.0</td>\n      <td>39.0</td>\n      <td>39.0</td>\n      <td>6.0</td>\n      <td>71.0</td>\n      <td>51.0</td>\n      <td>7.0</td>\n      <td>147.0</td>\n      <td>6.7415</td>\n      <td>2.0</td>\n      <td>0.014</td>\n      <td>0.019</td>\n      <td>0.0</td>\n      <td>57.0</td>\n      <td>46.0</td>\n      <td>1.00000</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>273.0</td>\n      <td>10.0</td>\n      <td>162.0</td>\n      <td>66.0</td>\n      <td>24.0</td>\n      <td>22.0</td>\n      <td>5.0</td>\n      <td>38.0</td>\n      <td>31.0</td>\n      <td>7.0</td>\n      <td>96.0</td>\n      <td>6.1898</td>\n      <td>4.0</td>\n      <td>0.042</td>\n      <td>0.039</td>\n      <td>0.0</td>\n      <td>23.0</td>\n      <td>38.0</td>\n      <td>0.00022</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>122.0</td>\n      <td>4.0</td>\n      <td>73.0</td>\n      <td>31.0</td>\n      <td>7.0</td>\n      <td>5.0</td>\n      <td>7.0</td>\n      <td>16.0</td>\n      <td>17.0</td>\n      <td>3.0</td>\n      <td>42.0</td>\n      <td>5.0000</td>\n      <td>1.0</td>\n      <td>0.024</td>\n      <td>0.094</td>\n      <td>0.0</td>\n      <td>15.0</td>\n      <td>7.0</td>\n      <td>0.95624</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]}]}