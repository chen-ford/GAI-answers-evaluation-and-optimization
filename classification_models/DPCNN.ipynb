{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-28T16:06:56.802519600Z",
     "start_time": "2024-04-28T16:06:56.154369600Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, MaxPooling1D, Conv1D, SpatialDropout1D\n",
    "from keras.layers import add, Dropout, PReLU, BatchNormalization, GlobalMaxPooling1D\n",
    "from keras import optimizers\n",
    "from keras import initializers, regularizers, constraints, callbacks\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "#读取数据集\n",
    "X_train = pd.read_csv('data/X_train_minmaxscaler.csv')['ChatGPT回答'].iloc[:100]\n",
    "X_test = pd.read_csv('data/X_test_minmaxscaler.csv')['ChatGPT回答'].iloc[:100]\n",
    "y_train = pd.read_csv('data/y_train_minmaxscaler.csv').iloc[:100,:]\n",
    "y_test = pd.read_csv('data/y_test_minmaxscaler.csv').iloc[:100,:]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T16:07:04.316717700Z",
     "start_time": "2024-04-28T16:06:57.515147Z"
    }
   },
   "id": "6b0a53bc7802aee0"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\mi\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.264 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "#处理数据\n",
    "\n",
    "cw = lambda x: list(jieba.cut(x))\n",
    "X_train_text = X_train.apply(cw)\n",
    "X_test_text = X_test.apply(cw)\n",
    "\n",
    "tokenizer=Tokenizer()  #创建一个Tokenizer对象\n",
    "#fit_on_texts函数可以将输入的文本中的每个词编号，编号是根据词频的，词频越大，编号越小\n",
    "tokenizer.fit_on_texts(X_train_text)\n",
    "tokenizer.fit_on_texts(X_test_text)\n",
    "vocab=tokenizer.word_index #得到每个词的编号\n",
    "\n",
    "# 将每个样本中的每个词转换为数字列表，使用每个词的编号进行编号\n",
    "X_train_word_ids=tokenizer.texts_to_sequences(X_train_text)\n",
    "X_test_word_ids = tokenizer.texts_to_sequences(X_test_text)\n",
    "#序列模式\n",
    "# 每条样本长度不唯一，将每条样本的长度设置一个固定值\n",
    "X_train_padded_seqs=pad_sequences(X_train_word_ids,maxlen=256) #将超过固定值的部分截掉，不足的在最前面用0填充\n",
    "X_test_padded_seqs=pad_sequences(X_test_word_ids, maxlen=256)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T16:07:46.019239600Z",
     "start_time": "2024-04-28T16:07:40.128739800Z"
    }
   },
   "id": "5284b9e91e4307b5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Smote不均衡采样\n",
    "from imblearn.over_sampling import SMOTE\n",
    "def smote(X_t, y_t):\n",
    "    # Create an instance of SMOTE\n",
    "    s = SMOTE(random_state=10)\n",
    "    # Apply SMOTE to the training data\n",
    "    X_train_resampled, y_train_resampled = s.fit_resample(X_t, y_t)\n",
    "    return X_train_resampled, y_train_resampled"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "45426a137a1a746b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#smote采样\n",
    "# X_train_padded_seqs, y_train = smote(X_train_padded_seqs, y_train)\n",
    "# X_test_padded_seqs, y_test = smote(X_test_padded_seqs, y_test)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16d25f02a2f6da0e"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "#model\n",
    "#wrote out all the blocks instead of looping for simplicity\n",
    "filter_nr = 64\n",
    "filter_size = 3\n",
    "max_pool_size = 3\n",
    "max_pool_strides = 2\n",
    "spatial_dropout = 0\n",
    "dense_dropout = 0.3\n",
    "train_embed = False\n",
    "conv_kern_reg = regularizers.l2(0.00001)\n",
    "conv_bias_reg = regularizers.l2(0.00001)\n",
    "\n",
    "comment = Input(shape=(256,), dtype='float64')\n",
    "emb_comment = Embedding(len(vocab) + 1, 300, input_length=256, trainable=False)(comment)\n",
    "emb_comment = SpatialDropout1D(spatial_dropout)(emb_comment)\n",
    "\n",
    "block1 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear', \n",
    "            kernel_regularizer=conv_kern_reg, bias_regularizer=conv_bias_reg)(emb_comment)\n",
    "block1 = BatchNormalization()(block1)\n",
    "block1 = PReLU()(block1)\n",
    "block1 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear', \n",
    "            kernel_regularizer=conv_kern_reg, bias_regularizer=conv_bias_reg)(block1)\n",
    "block1 = BatchNormalization()(block1)\n",
    "block1 = PReLU()(block1)\n",
    "\n",
    "#we pass embedded comment through conv1d with filter size 1 because it needs to have the same shape as block output\n",
    "#if you choose filter_nr = embed_size (300 in this case) you don't have to do this part and can add emb_comment directly to block1_output\n",
    "resize_emb = Conv1D(filter_nr, kernel_size=1, padding='same', activation='linear', \n",
    "            kernel_regularizer=conv_kern_reg, bias_regularizer=conv_bias_reg)(emb_comment)\n",
    "resize_emb = PReLU()(resize_emb)\n",
    "    \n",
    "block1_output = add([block1, resize_emb])\n",
    "block1_output = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(block1_output)\n",
    "\n",
    "block2 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear', \n",
    "            kernel_regularizer=conv_kern_reg, bias_regularizer=conv_bias_reg)(block1_output)\n",
    "block2 = BatchNormalization()(block2)\n",
    "block2 = PReLU()(block2)\n",
    "block2 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear', \n",
    "            kernel_regularizer=conv_kern_reg, bias_regularizer=conv_bias_reg)(block2)\n",
    "block2 = BatchNormalization()(block2)\n",
    "block2 = PReLU()(block2)\n",
    "    \n",
    "block2_output = add([block2, block1_output])\n",
    "block2_output = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(block2_output)\n",
    "\n",
    "block3 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear', \n",
    "            kernel_regularizer=conv_kern_reg, bias_regularizer=conv_bias_reg)(block2_output)\n",
    "block3 = BatchNormalization()(block3)\n",
    "block3 = PReLU()(block3)\n",
    "block3 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear', \n",
    "            kernel_regularizer=conv_kern_reg, bias_regularizer=conv_bias_reg)(block3)\n",
    "block3 = BatchNormalization()(block3)\n",
    "block3 = PReLU()(block3)\n",
    "    \n",
    "block3_output = add([block3, block2_output])\n",
    "output = GlobalMaxPooling1D()(block3_output)\n",
    "\n",
    "output = Dense(128, activation='linear')(output)\n",
    "output = BatchNormalization()(output)\n",
    "output = PReLU()(output)\n",
    "output = Dropout(dense_dropout)(output)\n",
    "output = Dense(1, activation='sigmoid')(output)\n",
    "\n",
    "model = Model(comment, output)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T16:20:27.765979200Z",
     "start_time": "2024-04-28T16:20:26.462293700Z"
    }
   },
   "id": "4556d01ce30f77fe"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error', \n",
    "            optimizer=optimizers.Adam(),\n",
    "            metrics=['accuracy'])\n",
    "            \n",
    "# Xtrain, Xval, ytrain, yval = train_test_split(x_train, y_train, train_size=0.95, random_state=233)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T16:20:28.638769Z",
     "start_time": "2024-04-28T16:20:28.624795100Z"
    }
   },
   "id": "68d17bc30f79079f"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, logs):  \n",
    "\n",
    "    # print(f'Epoch {epoch + 1}, Loss: {logs[\"loss\"]}') \n",
    "    print(f'Epoch {epoch + 1}') \n",
    "    train_loss = logs.get('loss')  \n",
    "    val_loss = logs.get('val_loss')  \n",
    "    train_acc = logs.get('accuracy')  \n",
    "    val_acc = logs.get('val_accuracy')  \n",
    "      \n",
    "    print(f'Epoch {epoch + 1}, Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, '  \n",
    "          f'Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}')  \n",
    "    print(\"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T16:20:29.314892300Z",
     "start_time": "2024-04-28T16:20:29.294334Z"
    }
   },
   "id": "edae6df77d1cd624"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from keras.src.callbacks import LambdaCallback\n",
    "\n",
    "# lr = callbacks.LearningRateScheduler(schedule)\n",
    "lr=0.0001\n",
    "callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "# ra_val = RocAucEvaluation(validation_data=(Xval, yval), interval = 1)\n",
    "history=model.fit(X_train_padded_seqs, y_train, batch_size=32, epochs=32, validation_split=0.2, callbacks = [callback] ,verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-30T10:51:42.185450300Z",
     "start_time": "2024-05-30T10:51:42.178351400Z"
    }
   },
   "id": "2cb8b3353bca059b"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# 保存整个模型到一个HDF5文件  \n",
    "# model.save('DPCNN_model.h5')  "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-30T10:51:42.255804900Z",
     "start_time": "2024-05-30T10:51:42.188456100Z"
    }
   },
   "id": "b8c1f86bab4253d0"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# 评估模型\n",
    "# score = model.evaluate(X_test_padded_seqs, y_test, verbose=0)  \n",
    "# print('Test loss:', score[0])  \n",
    "# print('Test accuracy:', score[1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-30T10:51:46.269870Z",
     "start_time": "2024-05-30T10:51:46.248873800Z"
    }
   },
   "id": "9e0b04d59f9d3a8d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    " # history.history 字典将包含每个epoch的loss和val_loss值  \n",
    "loss = history.history['loss']  \n",
    "val_loss = history.history['val_loss']\n",
    " \n",
    "# 绘制训练和验证loss曲线  \n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(loss, label='Training Loss')  \n",
    "plt.plot(val_loss, label='Validation Loss')  \n",
    "plt.title('Loss Curve')  \n",
    "plt.xlabel('Epoch')  \n",
    "plt.ylabel('Loss')  \n",
    "plt.legend()  \n",
    "#保存loss曲线\n",
    "# plt.savefig('DPCNN_loss_curve.png')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16384325c93f361a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#绘制准确率曲线\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "# plt.savefig('DPCNN_accuracy.png')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "75f442ca0e15fce4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#DPCNN模型的评价\n",
    "def evaluate_DPCNN_model(model, X_test, y_test):\n",
    "\n",
    "    # Predict probabilities\n",
    "    y_pred_proba = model.predict(X_test)\n",
    "    \n",
    "    # Predict labels\n",
    "    y_pred=np.where(y_pred_proba>0.5,1,0)\n",
    "    \n",
    "    \n",
    "    # Calculate accuracy, precision, recall, F1-score, and AUC\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    # Calculate ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "    \n",
    "    return [accuracy, precision, recall, f1, auc,fpr.tolist(), tpr.tolist()]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1ca00d55ec2b8c30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "# # 加载模型\n",
    "# def load_model(model_name):\n",
    "#     model = tf.keras.models.load_model(model_name)\n",
    "#     return model\n",
    "\n",
    "#计算每个模型的评价指标,\n",
    "metrics_name = ['accuracy', 'precision', 'recall', 'f1-score','auc','fpr-score','tpr-score']\n",
    "#计算每个模型的评价指标值，然后按照模型名，指标名称将结果存入一个字典\n",
    "\n",
    "metrics = evaluate_DPCNN_model(model, tf.convert_to_tensor(X_test_padded_seqs), y_test)\n",
    "DPCNN_metrics_dict = {metrics_name[j]: metrics[j] for j in range(len(metrics))}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc977c6a94958c1d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#以json文件保存字典结果\n",
    "with open('DPCNN_metrics_dict.json', 'w') as f:\n",
    "    json.dump(DPCNN_metrics_dict, f)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2edaec61f1861961"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
