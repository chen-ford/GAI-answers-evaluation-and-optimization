{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-28T16:06:56.802519600Z",
     "start_time": "2024-04-28T16:06:56.154369600Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, MaxPooling1D, Conv1D, SpatialDropout1D\n",
    "from keras.layers import add, Dropout, PReLU, BatchNormalization, GlobalMaxPooling1D\n",
    "from keras import optimizers\n",
    "from keras import initializers, regularizers, constraints, callbacks\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "#读取数据集\n",
    "X_train = pd.read_csv('data/X_train_minmaxscaler.csv')['ChatGPT回答'].iloc[:100]\n",
    "X_test = pd.read_csv('data/X_test_minmaxscaler.csv')['ChatGPT回答'].iloc[:100]\n",
    "y_train = pd.read_csv('data/y_train_minmaxscaler.csv').iloc[:100,:]\n",
    "y_test = pd.read_csv('data/y_test_minmaxscaler.csv').iloc[:100,:]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T16:07:04.316717700Z",
     "start_time": "2024-04-28T16:06:57.515147Z"
    }
   },
   "id": "6b0a53bc7802aee0"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\mi\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.264 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "#处理数据\n",
    "\n",
    "cw = lambda x: list(jieba.cut(x))\n",
    "X_train_text = X_train.apply(cw)\n",
    "X_test_text = X_test.apply(cw)\n",
    "\n",
    "tokenizer=Tokenizer()  #创建一个Tokenizer对象\n",
    "#fit_on_texts函数可以将输入的文本中的每个词编号，编号是根据词频的，词频越大，编号越小\n",
    "tokenizer.fit_on_texts(X_train_text)\n",
    "tokenizer.fit_on_texts(X_test_text)\n",
    "vocab=tokenizer.word_index #得到每个词的编号\n",
    "\n",
    "# 将每个样本中的每个词转换为数字列表，使用每个词的编号进行编号\n",
    "X_train_word_ids=tokenizer.texts_to_sequences(X_train_text)\n",
    "X_test_word_ids = tokenizer.texts_to_sequences(X_test_text)\n",
    "#序列模式\n",
    "# 每条样本长度不唯一，将每条样本的长度设置一个固定值\n",
    "X_train_padded_seqs=pad_sequences(X_train_word_ids,maxlen=256) #将超过固定值的部分截掉，不足的在最前面用0填充\n",
    "X_test_padded_seqs=pad_sequences(X_test_word_ids, maxlen=256)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T16:07:46.019239600Z",
     "start_time": "2024-04-28T16:07:40.128739800Z"
    }
   },
   "id": "5284b9e91e4307b5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Smote不均衡采样\n",
    "from imblearn.over_sampling import SMOTE\n",
    "def smote(X_t, y_t):\n",
    "    # Create an instance of SMOTE\n",
    "    s = SMOTE(random_state=10)\n",
    "    # Apply SMOTE to the training data\n",
    "    X_train_resampled, y_train_resampled = s.fit_resample(X_t, y_t)\n",
    "    return X_train_resampled, y_train_resampled"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "45426a137a1a746b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#smote采样\n",
    "# X_train_padded_seqs, y_train = smote(X_train_padded_seqs, y_train)\n",
    "# X_test_padded_seqs, y_test = smote(X_test_padded_seqs, y_test)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16d25f02a2f6da0e"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "#model\n",
    "#wrote out all the blocks instead of looping for simplicity\n",
    "filter_nr = 64\n",
    "filter_size = 3\n",
    "max_pool_size = 3\n",
    "max_pool_strides = 2\n",
    "spatial_dropout = 0\n",
    "dense_dropout = 0.3\n",
    "train_embed = False\n",
    "conv_kern_reg = regularizers.l2(0.00001)\n",
    "conv_bias_reg = regularizers.l2(0.00001)\n",
    "\n",
    "comment = Input(shape=(256,), dtype='float64')\n",
    "emb_comment = Embedding(len(vocab) + 1, 300, input_length=256, trainable=False)(comment)\n",
    "emb_comment = SpatialDropout1D(spatial_dropout)(emb_comment)\n",
    "\n",
    "block1 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear', \n",
    "            kernel_regularizer=conv_kern_reg, bias_regularizer=conv_bias_reg)(emb_comment)\n",
    "block1 = BatchNormalization()(block1)\n",
    "block1 = PReLU()(block1)\n",
    "block1 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear', \n",
    "            kernel_regularizer=conv_kern_reg, bias_regularizer=conv_bias_reg)(block1)\n",
    "block1 = BatchNormalization()(block1)\n",
    "block1 = PReLU()(block1)\n",
    "\n",
    "#we pass embedded comment through conv1d with filter size 1 because it needs to have the same shape as block output\n",
    "#if you choose filter_nr = embed_size (300 in this case) you don't have to do this part and can add emb_comment directly to block1_output\n",
    "resize_emb = Conv1D(filter_nr, kernel_size=1, padding='same', activation='linear', \n",
    "            kernel_regularizer=conv_kern_reg, bias_regularizer=conv_bias_reg)(emb_comment)\n",
    "resize_emb = PReLU()(resize_emb)\n",
    "    \n",
    "block1_output = add([block1, resize_emb])\n",
    "block1_output = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(block1_output)\n",
    "\n",
    "block2 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear', \n",
    "            kernel_regularizer=conv_kern_reg, bias_regularizer=conv_bias_reg)(block1_output)\n",
    "block2 = BatchNormalization()(block2)\n",
    "block2 = PReLU()(block2)\n",
    "block2 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear', \n",
    "            kernel_regularizer=conv_kern_reg, bias_regularizer=conv_bias_reg)(block2)\n",
    "block2 = BatchNormalization()(block2)\n",
    "block2 = PReLU()(block2)\n",
    "    \n",
    "block2_output = add([block2, block1_output])\n",
    "block2_output = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(block2_output)\n",
    "\n",
    "block3 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear', \n",
    "            kernel_regularizer=conv_kern_reg, bias_regularizer=conv_bias_reg)(block2_output)\n",
    "block3 = BatchNormalization()(block3)\n",
    "block3 = PReLU()(block3)\n",
    "block3 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear', \n",
    "            kernel_regularizer=conv_kern_reg, bias_regularizer=conv_bias_reg)(block3)\n",
    "block3 = BatchNormalization()(block3)\n",
    "block3 = PReLU()(block3)\n",
    "    \n",
    "block3_output = add([block3, block2_output])\n",
    "# block3_output = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(block3_output)\n",
    "# \n",
    "# block4 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear', \n",
    "#             kernel_regularizer=conv_kern_reg, bias_regularizer=conv_bias_reg)(block3_output)\n",
    "# block4 = BatchNormalization()(block4)\n",
    "# block4 = PReLU()(block4)\n",
    "# block4 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear', \n",
    "#             kernel_regularizer=conv_kern_reg, bias_regularizer=conv_bias_reg)(block4)\n",
    "# block4 = BatchNormalization()(block4)\n",
    "# block4 = PReLU()(block4)\n",
    "# \n",
    "# block4_output = add([block4, block3_output])\n",
    "# block4_output = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(block4_output)\n",
    "# \n",
    "# block5 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear', \n",
    "#             kernel_regularizer=conv_kern_reg, bias_regularizer=conv_bias_reg)(block4_output)\n",
    "# block5 = BatchNormalization()(block5)\n",
    "# block5 = PReLU()(block5)\n",
    "# block5 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear', \n",
    "#             kernel_regularizer=conv_kern_reg, bias_regularizer=conv_bias_reg)(block5)\n",
    "# block5 = BatchNormalization()(block5)\n",
    "# block5 = PReLU()(block5)\n",
    "# \n",
    "# block5_output = add([block5, block4_output])\n",
    "# block5_output = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(block5_output)\n",
    "# \n",
    "# block6 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear', \n",
    "#             kernel_regularizer=conv_kern_reg, bias_regularizer=conv_bias_reg)(block5_output)\n",
    "# block6 = BatchNormalization()(block6)\n",
    "# block6 = PReLU()(block6)\n",
    "# block6 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear', \n",
    "#             kernel_regularizer=conv_kern_reg, bias_regularizer=conv_bias_reg)(block6)\n",
    "# block6 = BatchNormalization()(block6)\n",
    "# block6 = PReLU()(block6)\n",
    "# \n",
    "# block6_output = add([block6, block5_output])\n",
    "# block6_output = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(block6_output)\n",
    "# \n",
    "# block7 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear', \n",
    "#             kernel_regularizer=conv_kern_reg, bias_regularizer=conv_bias_reg)(block6_output)\n",
    "# block7 = BatchNormalization()(block7)\n",
    "# block7 = PReLU()(block7)\n",
    "# block7 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear', \n",
    "#             kernel_regularizer=conv_kern_reg, bias_regularizer=conv_bias_reg)(block7)\n",
    "# block7 = BatchNormalization()(block7)\n",
    "# block7 = PReLU()(block7)\n",
    "# \n",
    "# block7_output = add([block7, block6_output])\n",
    "output = GlobalMaxPooling1D()(block3_output)\n",
    "\n",
    "output = Dense(128, activation='linear')(output)\n",
    "output = BatchNormalization()(output)\n",
    "output = PReLU()(output)\n",
    "output = Dropout(dense_dropout)(output)\n",
    "output = Dense(1, activation='sigmoid')(output)\n",
    "\n",
    "model = Model(comment, output)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T16:20:27.765979200Z",
     "start_time": "2024-04-28T16:20:26.462293700Z"
    }
   },
   "id": "4556d01ce30f77fe"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error', \n",
    "            optimizer=optimizers.Adam(),\n",
    "            metrics=['accuracy'])\n",
    "            \n",
    "# Xtrain, Xval, ytrain, yval = train_test_split(x_train, y_train, train_size=0.95, random_state=233)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T16:20:28.638769Z",
     "start_time": "2024-04-28T16:20:28.624795100Z"
    }
   },
   "id": "68d17bc30f79079f"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, logs):  \n",
    "\n",
    "    # print(f'Epoch {epoch + 1}, Loss: {logs[\"loss\"]}') \n",
    "    print(f'Epoch {epoch + 1}') \n",
    "    train_loss = logs.get('loss')  \n",
    "    val_loss = logs.get('val_loss')  \n",
    "    train_acc = logs.get('accuracy')  \n",
    "    val_acc = logs.get('val_accuracy')  \n",
    "      \n",
    "    print(f'Epoch {epoch + 1}, Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, '  \n",
    "          f'Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}')  \n",
    "    print(\"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T16:20:29.314892300Z",
     "start_time": "2024-04-28T16:20:29.294334Z"
    }
   },
   "id": "edae6df77d1cd624"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/32\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.3498 - accuracy: 0.4000Epoch 1\n",
      "Epoch 1, Loss: 0.3498, Val Loss: 0.2532, Acc: 0.4000, Val Acc: 0.6500\n",
      "\n",
      "\n",
      "3/3 [==============================] - 7s 436ms/step - loss: 0.3498 - accuracy: 0.4000 - val_loss: 0.2532 - val_accuracy: 0.6500\n",
      "Epoch 2/32\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2322 - accuracy: 0.6500Epoch 2\n",
      "Epoch 2, Loss: 0.2322, Val Loss: 0.2488, Acc: 0.6500, Val Acc: 0.6500\n",
      "\n",
      "\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 0.2322 - accuracy: 0.6500 - val_loss: 0.2488 - val_accuracy: 0.6500\n",
      "Epoch 3/32\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1549 - accuracy: 0.7875Epoch 3\n",
      "Epoch 3, Loss: 0.1549, Val Loss: 0.2405, Acc: 0.7875, Val Acc: 0.6500\n",
      "\n",
      "\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 0.1549 - accuracy: 0.7875 - val_loss: 0.2405 - val_accuracy: 0.6500\n",
      "Epoch 4/32\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0914 - accuracy: 0.9625Epoch 4\n",
      "Epoch 4, Loss: 0.0914, Val Loss: 0.2334, Acc: 0.9625, Val Acc: 0.6500\n",
      "\n",
      "\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.0914 - accuracy: 0.9625 - val_loss: 0.2334 - val_accuracy: 0.6500\n",
      "Epoch 5/32\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0613 - accuracy: 0.9875Epoch 5\n",
      "Epoch 5, Loss: 0.0613, Val Loss: 0.2318, Acc: 0.9875, Val Acc: 0.6500\n",
      "\n",
      "\n",
      "3/3 [==============================] - 0s 149ms/step - loss: 0.0613 - accuracy: 0.9875 - val_loss: 0.2318 - val_accuracy: 0.6500\n",
      "Epoch 6/32\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0527 - accuracy: 0.9750Epoch 6\n",
      "Epoch 6, Loss: 0.0527, Val Loss: 0.2382, Acc: 0.9750, Val Acc: 0.6500\n",
      "\n",
      "\n",
      "3/3 [==============================] - 0s 136ms/step - loss: 0.0527 - accuracy: 0.9750 - val_loss: 0.2382 - val_accuracy: 0.6500\n",
      "Epoch 7/32\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0410 - accuracy: 0.9875Epoch 7\n",
      "Epoch 7, Loss: 0.0410, Val Loss: 0.2516, Acc: 0.9875, Val Acc: 0.6500\n",
      "\n",
      "\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.0410 - accuracy: 0.9875 - val_loss: 0.2516 - val_accuracy: 0.6500\n",
      "Epoch 8/32\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0280 - accuracy: 1.0000Epoch 8\n",
      "Epoch 8, Loss: 0.0280, Val Loss: 0.2693, Acc: 1.0000, Val Acc: 0.6500\n",
      "\n",
      "\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 0.0280 - accuracy: 1.0000 - val_loss: 0.2693 - val_accuracy: 0.6500\n",
      "Epoch 9/32\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0192 - accuracy: 1.0000Epoch 9\n",
      "Epoch 9, Loss: 0.0192, Val Loss: 0.2869, Acc: 1.0000, Val Acc: 0.6500\n",
      "\n",
      "\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.0192 - accuracy: 1.0000 - val_loss: 0.2869 - val_accuracy: 0.6500\n",
      "Epoch 10/32\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0202 - accuracy: 1.0000Epoch 10\n",
      "Epoch 10, Loss: 0.0202, Val Loss: 0.3032, Acc: 1.0000, Val Acc: 0.6500\n",
      "\n",
      "\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 0.0202 - accuracy: 1.0000 - val_loss: 0.3032 - val_accuracy: 0.6500\n",
      "Epoch 11/32\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 1.0000Epoch 11\n",
      "Epoch 11, Loss: 0.0169, Val Loss: 0.3168, Acc: 1.0000, Val Acc: 0.6500\n",
      "\n",
      "\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 0.0169 - accuracy: 1.0000 - val_loss: 0.3168 - val_accuracy: 0.6500\n",
      "Epoch 12/32\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0150 - accuracy: 1.0000Epoch 12\n",
      "Epoch 12, Loss: 0.0150, Val Loss: 0.3275, Acc: 1.0000, Val Acc: 0.6500\n",
      "\n",
      "\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 0.0150 - accuracy: 1.0000 - val_loss: 0.3275 - val_accuracy: 0.6500\n",
      "Epoch 13/32\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0116 - accuracy: 1.0000Epoch 13\n",
      "Epoch 13, Loss: 0.0116, Val Loss: 0.3356, Acc: 1.0000, Val Acc: 0.6500\n",
      "\n",
      "\n",
      "3/3 [==============================] - 0s 144ms/step - loss: 0.0116 - accuracy: 1.0000 - val_loss: 0.3356 - val_accuracy: 0.6500\n",
      "Epoch 14/32\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0110 - accuracy: 1.0000Epoch 14\n",
      "Epoch 14, Loss: 0.0110, Val Loss: 0.3415, Acc: 1.0000, Val Acc: 0.6500\n",
      "\n",
      "\n",
      "3/3 [==============================] - 0s 151ms/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 0.3415 - val_accuracy: 0.6500\n",
      "Epoch 15/32\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0146 - accuracy: 1.0000Epoch 15\n",
      "Epoch 15, Loss: 0.0146, Val Loss: 0.3458, Acc: 1.0000, Val Acc: 0.6500\n",
      "\n",
      "\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 0.0146 - accuracy: 1.0000 - val_loss: 0.3458 - val_accuracy: 0.6500\n",
      "Epoch 16/32\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0102 - accuracy: 1.0000Epoch 16\n",
      "Epoch 16, Loss: 0.0102, Val Loss: 0.3487, Acc: 1.0000, Val Acc: 0.6500\n",
      "\n",
      "\n",
      "3/3 [==============================] - 0s 136ms/step - loss: 0.0102 - accuracy: 1.0000 - val_loss: 0.3487 - val_accuracy: 0.6500\n",
      "Epoch 17/32\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0094 - accuracy: 1.0000Epoch 17\n",
      "Epoch 17, Loss: 0.0094, Val Loss: 0.3507, Acc: 1.0000, Val Acc: 0.6500\n",
      "\n",
      "\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 0.3507 - val_accuracy: 0.6500\n",
      "Epoch 18/32\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0141 - accuracy: 0.9875Epoch 18\n",
      "Epoch 18, Loss: 0.0141, Val Loss: 0.3522, Acc: 0.9875, Val Acc: 0.6500\n",
      "\n",
      "\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.0141 - accuracy: 0.9875 - val_loss: 0.3522 - val_accuracy: 0.6500\n",
      "Epoch 19/32\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0115 - accuracy: 1.0000Epoch 19\n",
      "Epoch 19, Loss: 0.0115, Val Loss: 0.3533, Acc: 1.0000, Val Acc: 0.6500\n",
      "\n",
      "\n",
      "3/3 [==============================] - 0s 142ms/step - loss: 0.0115 - accuracy: 1.0000 - val_loss: 0.3533 - val_accuracy: 0.6500\n",
      "Epoch 20/32\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0078 - accuracy: 1.0000Epoch 20\n",
      "Epoch 20, Loss: 0.0078, Val Loss: 0.3539, Acc: 1.0000, Val Acc: 0.6500\n",
      "\n",
      "\n",
      "3/3 [==============================] - 0s 140ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.3539 - val_accuracy: 0.6500\n",
      "Epoch 21/32\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0082 - accuracy: 1.0000Epoch 21\n",
      "Epoch 21, Loss: 0.0082, Val Loss: 0.3544, Acc: 1.0000, Val Acc: 0.6500\n",
      "\n",
      "\n",
      "3/3 [==============================] - 0s 140ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.3544 - val_accuracy: 0.6500\n",
      "Epoch 22/32\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0081 - accuracy: 1.0000Epoch 22\n",
      "Epoch 22, Loss: 0.0081, Val Loss: 0.3546, Acc: 1.0000, Val Acc: 0.6500\n",
      "\n",
      "\n",
      "3/3 [==============================] - 0s 137ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.3546 - val_accuracy: 0.6500\n",
      "Epoch 23/32\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0116 - accuracy: 1.0000Epoch 23\n",
      "Epoch 23, Loss: 0.0116, Val Loss: 0.3548, Acc: 1.0000, Val Acc: 0.6500\n",
      "\n",
      "\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 0.0116 - accuracy: 1.0000 - val_loss: 0.3548 - val_accuracy: 0.6500\n",
      "Epoch 24/32\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0085 - accuracy: 1.0000Epoch 24\n",
      "Epoch 24, Loss: 0.0085, Val Loss: 0.3550, Acc: 1.0000, Val Acc: 0.6500\n",
      "\n",
      "\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.3550 - val_accuracy: 0.6500\n",
      "Epoch 25/32\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0097 - accuracy: 1.0000Epoch 25\n",
      "Epoch 25, Loss: 0.0097, Val Loss: 0.3551, Acc: 1.0000, Val Acc: 0.6500\n",
      "\n",
      "\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 0.3551 - val_accuracy: 0.6500\n",
      "Epoch 26/32\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0076 - accuracy: 1.0000Epoch 26\n",
      "Epoch 26, Loss: 0.0076, Val Loss: 0.3551, Acc: 1.0000, Val Acc: 0.6500\n",
      "\n",
      "\n",
      "3/3 [==============================] - 0s 139ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.3551 - val_accuracy: 0.6500\n",
      "Epoch 27/32\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0083 - accuracy: 1.0000Epoch 27\n",
      "Epoch 27, Loss: 0.0083, Val Loss: 0.3552, Acc: 1.0000, Val Acc: 0.6500\n",
      "\n",
      "\n",
      "3/3 [==============================] - 0s 139ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.3552 - val_accuracy: 0.6500\n",
      "Epoch 28/32\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0072 - accuracy: 1.0000Epoch 28\n",
      "Epoch 28, Loss: 0.0072, Val Loss: 0.3552, Acc: 1.0000, Val Acc: 0.6500\n",
      "\n",
      "\n",
      "3/3 [==============================] - 0s 137ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.3552 - val_accuracy: 0.6500\n",
      "Epoch 29/32\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0097 - accuracy: 1.0000Epoch 29\n",
      "Epoch 29, Loss: 0.0097, Val Loss: 0.3553, Acc: 1.0000, Val Acc: 0.6500\n",
      "\n",
      "\n",
      "3/3 [==============================] - 0s 139ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 0.3553 - val_accuracy: 0.6500\n",
      "Epoch 30/32\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0085 - accuracy: 1.0000Epoch 30\n",
      "Epoch 30, Loss: 0.0085, Val Loss: 0.3553, Acc: 1.0000, Val Acc: 0.6500\n",
      "\n",
      "\n",
      "3/3 [==============================] - 0s 160ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.3553 - val_accuracy: 0.6500\n",
      "Epoch 31/32\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0076 - accuracy: 1.0000Epoch 31\n",
      "Epoch 31, Loss: 0.0076, Val Loss: 0.3553, Acc: 1.0000, Val Acc: 0.6500\n",
      "\n",
      "\n",
      "3/3 [==============================] - 0s 158ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.3553 - val_accuracy: 0.6500\n",
      "Epoch 32/32\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0067 - accuracy: 1.0000Epoch 32\n",
      "Epoch 32, Loss: 0.0067, Val Loss: 0.3553, Acc: 1.0000, Val Acc: 0.6500\n",
      "\n",
      "\n",
      "3/3 [==============================] - 0s 154ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.3553 - val_accuracy: 0.6500\n"
     ]
    }
   ],
   "source": [
    "from keras.src.callbacks import LambdaCallback\n",
    "\n",
    "# lr = callbacks.LearningRateScheduler(schedule)\n",
    "lr=0.0001\n",
    "callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "# ra_val = RocAucEvaluation(validation_data=(Xval, yval), interval = 1)\n",
    "history=model.fit(X_train_padded_seqs, y_train, batch_size=32, epochs=32, validation_split=0.2, callbacks = [callback] ,verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T16:20:50.863731600Z",
     "start_time": "2024-04-28T16:20:31.014207500Z"
    }
   },
   "id": "2cb8b3353bca059b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 保存整个模型到一个HDF5文件  \n",
    "# model.save('DPCNN_model.h5')  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8c1f86bab4253d0"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.2153167724609375\n",
      "Test accuracy: 0.7900000214576721\n"
     ]
    }
   ],
   "source": [
    "# 评估模型\n",
    "score = model.evaluate(X_test_padded_seqs, y_test, verbose=0)  \n",
    "print('Test loss:', score[0])  \n",
    "print('Test accuracy:', score[1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T16:20:54.010531600Z",
     "start_time": "2024-04-28T16:20:53.811778600Z"
    }
   },
   "id": "9e0b04d59f9d3a8d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    " # history.history 字典将包含每个epoch的loss和val_loss值  \n",
    "loss = history.history['loss']  \n",
    "val_loss = history.history['val_loss']\n",
    " \n",
    "# 绘制训练和验证loss曲线  \n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(loss, label='Training Loss')  \n",
    "plt.plot(val_loss, label='Validation Loss')  \n",
    "plt.title('Loss Curve')  \n",
    "plt.xlabel('Epoch')  \n",
    "plt.ylabel('Loss')  \n",
    "plt.legend()  \n",
    "#保存loss曲线\n",
    "# plt.savefig('DPCNN_loss_curve.png')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16384325c93f361a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#绘制准确率曲线\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "# plt.savefig('DPCNN_accuracy.png')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "75f442ca0e15fce4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#DPCNN模型的评价\n",
    "def evaluate_DPCNN_model(model, X_test, y_test):\n",
    "\n",
    "    # Predict probabilities\n",
    "    y_pred_proba = model.predict(X_test)\n",
    "    \n",
    "    # Predict labels\n",
    "    y_pred=np.where(y_pred_proba>0.5,1,0)\n",
    "    \n",
    "    \n",
    "    # Calculate accuracy, precision, recall, F1-score, and AUC\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    # Calculate ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "    \n",
    "    return [accuracy, precision, recall, f1, auc,fpr.tolist(), tpr.tolist()]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1ca00d55ec2b8c30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "# # 加载模型\n",
    "# def load_model(model_name):\n",
    "#     model = tf.keras.models.load_model(model_name)\n",
    "#     return model\n",
    "\n",
    "#计算每个模型的评价指标,\n",
    "metrics_name = ['accuracy', 'precision', 'recall', 'f1-score','auc','fpr-score','tpr-score']\n",
    "#计算每个模型的评价指标值，然后按照模型名，指标名称将结果存入一个字典\n",
    "\n",
    "metrics = evaluate_DPCNN_model(model, tf.convert_to_tensor(X_test_padded_seqs), y_test)\n",
    "DPCNN_metrics_dict = {metrics_name[j]: metrics[j] for j in range(len(metrics))}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc977c6a94958c1d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#以json文件保存字典结果\n",
    "with open('DPCNN_metrics_dict.json', 'w') as f:\n",
    "    json.dump(DPCNN_metrics_dict, f)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2edaec61f1861961"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
